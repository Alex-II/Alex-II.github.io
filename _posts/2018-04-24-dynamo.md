---
layout: post
title:  "Dynamo: Amazon’s Highly Available Key-value Store"
category: papers
---

# [Dynamo: Amazon’s Highly Available Key-value Store](https://web.stanford.edu/class/cs340v/papers/dynamo.pdf)

<u>Amazon Services DB Needs</u>  
Dynamo presents the idea of a non-relational databases tailored to the needs of Amazon services. Amazon observed that the traditional relational databases present the wrong set of features for too high a price.   
Many Amazon services need to frequently store and retrieve small amounts data associated with an identity; they have no need for a complex schema or of many of the features a relational database offers, they simply want a key-value store.   
Furthermore, the availability of the key-value store is more important to these services than it's guaranteed consistency, which departs again from traditional database expectations.   
Lastly scalability, also of high priority, cannot be easily achieved with traditional relational databases. This is due to the guarantees and large feature set that relational databases that do not fit the needs of Amazon's services.


Using traditional databases to fit these needs would be convoluted (and so prone to errors), expensive to operate and probably still not meet the desired highly available and easily scalable objectives.

The authors of the paper also explore the importance of Service Level Agreements (SLA) that Dynamo intends to meet. Indeed, Amazon has stringent performance expectations of their services and, consequently, of the underlying storage.  
It seems that all other distributed key-value store had chosen tradeoffs that either made reaching the SLAs hard to attain or encumbered the key-value store with features that add unneeded complexity to the system.  
For example, Dynamo would be running in a trusted environment, which means that systems considering authorization, authentication or Byzantine failure were doing too much.
Dynamo would not need the (relatively) complex storage schema offered by Google's BigTable nor would it need to ensure complex data reconciliation when consistency eventually becomes an issue.

We can say that Dynamo's goals are specialized: key-value store with high performance, high availability and scalability. To attain them, they were willing to make tradeoffs with consistency, forego many database features and shift more responsibility to the users of the store.

<u>Usage</u>  
Dynamo has a simple interface with put() and get() operations.  
The value is treated as a blob, and the key is hashed to create a 128-bit ID.  

The get() operations is a read operation, returning data but also a context.  
The context carries the version info (as vector clocks) of the data being retrieved.  
When using put() to update the value of a given key, put() expects the context the user was given from the last get().  

<u>Data Reconciliation</u>  
The version info in the context carried by the user between gets and puts is used to allow for syntactical reconciliation; essentially, if a causal relationship can be inferred between data stored on different nodes, the system can decide which data is truly canonical and discard the other.  
This syntactical reconciliation happens on a get operation.  

The most interesting responsibility shift is more the complex semantic reconciliation: if Dynamo cannot use the context/version info to perform syntactical reconciliation, it asks the user to do achieve semantic reconciliation.   
Thus, when performing a get, a user may be receive several versions of the data; it's up to the user to reconcile the data. This makes sense, given the business logic would know best how to best reconcile imperfectly consistent data in the least 'damaging' way.  
The example given for semantic reconciliation is  shopping cart, where a get operation returns two shopping carts.  
The user's semantic reconciliation can just be adding missing items for one cart to the other.  

Dynamo prefers to shift data reconciliation complexity to reads, making the system write friendly.

<u>Partitioning</u>  
Data is partitioned and replicated among the cluster nodes using consistent hashing in a ring formation, as explained in detail in the Chord paper.    
A read or write operation for a particular key is handled by a coordinator.  For write operations, the coordinator is one node from a set of nodes designated as "responsible" for a particular  key (the coordinators are derived from hashes of the nodes' identity and a hash of the key). For read operations, any node can be a coordinator.  
On a read or write request, the coordinator talks to other nodes "responsible" for the key, either reading from several of them (in case one of the nodes has a more recent version) or asking them to write the data for the key, and only reporting back to the caller once enough nodes have confirmed to the coordinator that they have written the data locally.

If a node, not "responsible" for a key but receives a write request, it will not act as the coordinator and instead forward the request to one of the healthy "responsible" nodes for that key; one of those will be the coordinator for that write request.

<u>High Availability</u>  
Dynamo achieves high availability by making all nodes symmetrical (e.g. no special master node, all nodes are coordinators for some set of keys), diminishing points of failure.  
It also attempts to replicate data across diverse hosts located in different data centers, again diminishing points of failure.  
While each key(-value pair) is assigned to certain nodes, if those preferred nodes are unavailable, other less preferable nodes store a write and periodically check for the preferred nodes so as to transfer them the data they missed while unavailable (hinted handoff).  

<u>Tuning</u>  
An interesting choice let to the user configure N, R and W, where  
N: total number of nodes that should store copies of the data   
W: minimum number of nodes that report a successful write before a put() operation is considered completed  
R: minimum number of nodes that return data for a given key before the result is returned to the user  

For W, we still try to eventually write the data to all N responsible for a given key, but for performance reasons, the user might be happy to know only a few (only W) have written the data and that, eventually, later, all N nodes will write the data.  

For R, we query all N nodes for data and we are satisfied to return the data we've gotten so far when we've received R responses.
This is where different versions of the same data is returned to the user, if the system isn't able to perform syntactic reconciliation. 

The coordinator coordinates the writing to all N nodes (and returns success when W have written) and coordinates reading from R nodes (and returns the data itself).

This means that the user can evaluate and decide on the importance of availability
Setting W to 1 will return rapidly but increases the chances data loss if the 1 node fails catastrophically after confirming the write but before the data is replicated to other nodes.
Similarly, setting R to 1 also returns rapidly but increases the change of getting stale data.

Common (N, R, W) values are (3,2,2)

<u>Adding/Removing Nodes</u>  
Dynamo does not automatically add/remove nodes from the cluster: adding/removing requires human intervention.  
Nodes use a gossip protocol to communicate membership changes as well as communicate which key(-value pairs) a particular node is responsible for.  

<u>High Performance Tricks</u>  
+ Writing buffer: a write is considered successful once it reaches main memory (and is scheduled to be written to disk). The coordinator does ask one of the N nodes to perform a durable, on-disk write before considering the write successful. This durable write does not affect performance because generally W < N so that W nodes will quickly report the write complete to the coordinator while the last node will churn along actually writing to disk; the coordinator doesn't need to wait after that last node to report the write complete to the user.
+ Uniform load distribution: Dynamo uses different partitioning strategies to ensure uniform load across all nodes, as the cost of more coordination when adding/removing nodes.
+ Writing to fastest reader: the context token returned from a read operation indicates the node with the fastest response time for that operation. This same node can then be used as the write coordinator for any upcoming put requests. Hopefully, the node will once more be the fastest to be reached and might also be read from next read (thus, the user will read their latest write).
+ Cluster-aware clients: If the Dynamo clients are aware of the nodes in the cluster, the data partition/key distribution among the nodes, and regularly update their view of the system, requests don't need to go through a load-balancer. Indeed, client requests can directly go to the appropriate coordinator for reads or writes.
+ Heterogeneous hardware: the nodes can take on requests proportional to their hardware capabilities; not requiring homogeneous hardware means hardware resources aren't wasted if more powerful machines are added to the cluster, and updates of cluster nodes can be performed progressively. 
+ Zero-hop distributed hash table: (unlike Chord) each node maintains routing information to reach any other node directly; more to maintain, smaller latency given less hops to perform
+ Foreground operations priority: foreground tasks (put/get operations) take precedence over background tasks (updating membership with neighbors, hinted handoff). A monitoring process on each node throttles background tasks based on resource utilization on the machine.